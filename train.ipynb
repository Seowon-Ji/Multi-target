{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMdE9J7oLLhy+hEZmhqvrLA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seowon-Ji/Multi-target/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMp67IdrogB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import argparse\n",
        "import random\n",
        "import model\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "from dataloader import goprodataset\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torch import autograd\n",
        "\n",
        "from utils import VisdomLinePlotter\n",
        "from skimage.measure import compare_psnr, compare_ssim\n",
        "from vgg import Vgg19, normalize_vgg\n",
        "\n",
        "from losses import init_loss\n",
        "import losses\n",
        "from options.train_options import TrainOptions\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Deep Multi-Patch Hierarchical Network\")\n",
        "parser.add_argument(\"-e\", \"--epochs\", type=int, default=1200)\n",
        "parser.add_argument(\"-se\", \"--start_epoch\", type=int, default=0)\n",
        "parser.add_argument(\"-b\", \"--batchsize\", type=int, default=8)\n",
        "parser.add_argument(\"-s\", \"--imagesize\", type=int, default=256)\n",
        "parser.add_argument(\"-l\", \"--learning_rate\", type=float, default=0.0001)\n",
        "parser.add_argument(\"-g\", \"--gpu\", type=int, default=0)\n",
        "parser.add_argument(\"-save\", \"--save_file\", type=bool, default=False)\n",
        "parser.add_argument(\"-visdom\", \"--visdom\", type=bool, default=False)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Hyper Parameters\n",
        "METHOD = \"DMPHN_1_2_4_8_l1_gan\"\n",
        "LEARNING_RATE = args.learning_rate\n",
        "EPOCHS = args.epochs\n",
        "GPU = args.gpu\n",
        "BATCH_SIZE = args.batchsize\n",
        "IMAGE_SIZE = args.imagesize\n",
        "SAVE_FILE = args.save_file\n",
        "VISDOM = args.visdom\n",
        "\n",
        "opt = TrainOptions().parse()\n",
        "opt.dataroot = 'D:\\Photos\\TrainingData\\BlurredSharp\\combined'\n",
        "opt.learn_residual = True\n",
        "opt.resize_or_crop = \"crop\"\n",
        "opt.fineSize = 256\n",
        "opt.gan_type = \"gan\"\n",
        "# opt.which_model_netG = \"unet_256\"\n",
        "\n",
        "# default = 5000\n",
        "opt.save_latest_freq = 100\n",
        "\n",
        "# default = 100\n",
        "opt.print_freq = 20\n",
        "\n",
        "opt = TrainOptions().parse()\n",
        "opt.dataroot = 'D:\\Photos\\TrainingData\\BlurredSharp\\combined'\n",
        "opt.learn_residual = True\n",
        "opt.resize_or_crop = \"crop\"\n",
        "opt.fineSize = 256\n",
        "opt.gan_type = \"gan\"\n",
        "\n",
        "def save_deblur_images(images, iteration, epoch):\n",
        "    filename_list = './checkpoints/' + METHOD + \"/epoch\" + str(epoch)\n",
        "    if not os.path.exists(filename_list):\n",
        "        os.makedirs(filename_list)\n",
        "    filename = './checkpoints/' + METHOD + \"/epoch\" + str(epoch) + \"/\" + \"Iter_\" + str(iteration) + \"_deblur.png\"\n",
        "    torchvision.utils.save_image(images, filename)\n",
        "\n",
        "\n",
        "def weight_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0.0, 0.5 * math.sqrt(2. / n))\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "    elif classname.find('Linear') != -1:\n",
        "        n = m.weight.size(1)\n",
        "        m.weight.data.normal_(0, 0.01)\n",
        "        m.bias.data = torch.ones(m.bias.data.size())\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    if VISDOM == True:\n",
        "        plotter_valid = VisdomLinePlotter(env_name='Plots')\n",
        "\n",
        "    if os.path.exists(str('./checkpoints/' + METHOD + \"/epoch.pkl\")):\n",
        "        checkpoint = torch.load(str('./checkpoints/' + METHOD + \"/epoch.pkl\"))\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        curr_lr = LEARNING_RATE * (0.1 ** (start_epoch // 550))\n",
        "        print(\"loaded epoch %d\" % start_epoch)\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "        curr_lr = LEARNING_RATE\n",
        "\n",
        "    print(\"init data folders\")\n",
        "\n",
        "    netG = model.Generator()\n",
        "    # netD = model.Discriminator_deblurganv1()\n",
        "    netD = model.NLayerDiscriminator()\n",
        "    netG.apply(weight_init).cuda()\n",
        "    netD.apply(weight_init).cuda()\n",
        "    criticUpdates = 1\n",
        "\n",
        "    step_size = 550\n",
        "    Tensor = torch.cuda.FloatTensor\n",
        "    # define loss functions\n",
        "    discLoss, contentLoss = init_loss(opt, Tensor)\n",
        "\n",
        "    netG_optim = torch.optim.Adam(netG.parameters(), lr=curr_lr)\n",
        "    netG_scheduler = StepLR(netG_optim, step_size=step_size, gamma=0.1)\n",
        "    netD_optim = torch.optim.Adam(netD.parameters(), lr=curr_lr)\n",
        "    netD_scheduler = StepLR(netD_optim, step_size=step_size, gamma=0.1)\n",
        "\n",
        "\n",
        "    if os.path.exists(str('./checkpoints/' + METHOD + \"/netG.pkl\")):\n",
        "        netG.load_state_dict(torch.load(str('./checkpoints/' + METHOD + \"/netG.pkl\")))\n",
        "        print(\"load netG success\")\n",
        "    if os.path.exists(str('./checkpoints/' + METHOD + \"/netD.pkl\")):\n",
        "        netG.load_state_dict(torch.load(str('./checkpoints/' + METHOD + \"/netD.pkl\")))\n",
        "        print(\"load netD success\")\n",
        "\n",
        "    if os.path.exists('./checkpoints/' + METHOD) == False:\n",
        "        os.system('mkdir ./checkpoints/' + METHOD)\n",
        "\n",
        "    for epoch in range(args.start_epoch, EPOCHS):\n",
        "        netG.train()\n",
        "        netD.train()\n",
        "\n",
        "        netG_scheduler.step(epoch)\n",
        "        netD_scheduler.step(epoch)\n",
        "\n",
        "        print(\"Training...\")\n",
        "\n",
        "        train_dataset = goprodataset(\n",
        "            blur_dir='../deblur_data/gopro_reset/train/blur',\n",
        "            sharp_dir='../deblur_data/gopro_reset/train/sharp',\n",
        "            crop=True,\n",
        "            crop_size=IMAGE_SIZE,\n",
        "            # transform=transforms.ToTensor()\n",
        "        )\n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        for param_group in netG_optim.param_groups:\n",
        "            print(\"netG_current lr: %f\"%param_group['lr'])\n",
        "        for param_group in netD_optim.param_groups:\n",
        "            print(\"netD_current lr: %f\"%param_group['lr'])\n",
        "\n",
        "        start = time.time()\n",
        "        for iteration, images in enumerate(tqdm(train_dataloader)):\n",
        "            # break\n",
        "            # mse = nn.MSELoss().cuda(GPU)\n",
        "            # l1 = nn.L1Loss().cuda(GPU)\n",
        "\n",
        "            gt = Variable(images['sharp_image'] - 0.5).cuda(GPU)\n",
        "            images_lv1 = Variable(images['blur_image'] - 0.5).cuda(GPU)\n",
        "\n",
        "\n",
        "            deblur_image = netG(images_lv1)\n",
        "            ############################\n",
        "            # (1) Update D network\n",
        "            ###########################\n",
        "            for iter_d in range(criticUpdates):\n",
        "                netD_optim.zero_grad()\n",
        "                # loss_D = self.discLoss.get_loss(netD, self.real_A, self.fake_B, self.real_B)\n",
        "                loss_D =discLoss.get_loss(netD, images_lv1, deblur_image, gt)\n",
        "                loss_D.backward(retain_graph=True)\n",
        "                netD_optim.step()\n",
        "\n",
        "            ############################\n",
        "            # (2) Update G network\n",
        "            ###########################\n",
        "            a = 0.01\n",
        "            netG_optim.zero_grad()\n",
        "            # loss_G_GAN = discLoss.get_g_loss(netD, self.real_A, self.fake_B)\n",
        "            loss_G_GAN = discLoss.get_g_loss(netD, images_lv1, deblur_image)\n",
        "            # Second, G(A) = B\n",
        "            # loss_G_Content = contentLoss.get_loss(self.fake_B, self.real_B) * self.opt.lambda_A\n",
        "            loss_G_Content = contentLoss.get_loss(deblur_image, gt)\n",
        "\n",
        "            loss_G = a * loss_G_GAN + loss_G_Content\n",
        "\n",
        "            loss_G.backward()\n",
        "            netG_optim.step()\n",
        "            # pixel_loss = mse(deblur_image, gt)\n",
        "\n",
        "\n",
        "            if (iteration + 1) % 10 == 0:\n",
        "                stop = time.time()\n",
        "                print(\"epoch:\", epoch, \"iteration:\", iteration + 1, \"loss:%.4f\" % loss_G.item(),\"adversarial_loss:%.4f\"%loss_G_GAN.item(),\"deblur_dis:%.4f\"%loss_D,\n",
        "                      'time:%.4f' % (stop - start))\n",
        "                start = time.time()\n",
        "\n",
        "        if (epoch) % 10 == 0:\n",
        "            if os.path.exists('./checkpoints/' + METHOD + '/epoch' + str(epoch)) == False:\n",
        "                os.system('mkdir ./checkpoints/' + METHOD + '/epoch' + str(epoch))\n",
        "\n",
        "            if not os.path.exists('./checkpoints/' + METHOD +'/parameter' + '/epoch' + str(epoch)):\n",
        "                os.makedirs('./checkpoints/' + METHOD +'/parameter' + '/epoch' + str(epoch))\n",
        "\n",
        "            torch.save(netG.state_dict(), str('./checkpoints/' + METHOD +'/parameter' + '/epoch' + str(epoch) + \"/netG.pkl\"))\n",
        "            torch.save(netD.state_dict(), str('./checkpoints/' + METHOD +'/parameter' + '/epoch' + str(epoch) + \"/netD.pkl\"))\n",
        "\n",
        "            torch.save({'epoch': epoch}, str('./checkpoints/' + METHOD +'/parameter' + '/epoch' + str(epoch) + \"/epoch.pkl\"))\n",
        "\n",
        "\n",
        "            print(\"valid...\")\n",
        "            valid_dataset =  goprodataset(\n",
        "                blur_dir='../deblur_data/gopro_reset/valid/blur',\n",
        "                sharp_dir='../deblur_data/gopro_reset/valid/sharp'\n",
        "                )\n",
        "            valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
        "            test_time = 0\n",
        "            total_psnr = []\n",
        "            total_ssim = []\n",
        "\n",
        "            netG.eval()\n",
        "            for iteration, images in enumerate(tqdm(valid_dataloader)):\n",
        "                with torch.no_grad():\n",
        "                    start = time.time()\n",
        "                    images_lv1 = Variable(images['blur_image'] - 0.5).cuda(GPU)\n",
        "                    deblur_image = netG(images_lv1)\n",
        "                    stop = time.time()\n",
        "                    test_time += stop - start\n",
        "                    # print('RunTime:%.4f' % (stop - start), '  Average Runtime:%.4f' % (test_time / (iteration + 1)))\n",
        "                    if SAVE_FILE ==True:\n",
        "                        save_deblur_images(deblur_image.data + 0.5, iteration, epoch)\n",
        "\n",
        "                    # numpy array\n",
        "                    deblur_image.data = deblur_image.data + 0.5\n",
        "                    out_np = deblur_image.cpu().numpy()\n",
        "                    rgb_images_np = images['sharp_image'].numpy()\n",
        "                    for i in range(deblur_image.size(0)):\n",
        "                        index = iteration + i\n",
        "\n",
        "                        psnr = compare_psnr(im_true=rgb_images_np[i], im_test=out_np[i])\n",
        "                        ssim = (compare_ssim(X=rgb_images_np[i][0], Y=out_np[i][0]) +\n",
        "                                compare_ssim(X=rgb_images_np[i][1], Y=out_np[i][1]) +\n",
        "                                compare_ssim(X=rgb_images_np[i][2], Y=out_np[i][2])) / 3\n",
        "                        total_psnr.append(psnr)\n",
        "                        total_ssim.append(ssim)\n",
        "\n",
        "            print('RunTime:%.4f' % (stop - start), '  Average Runtime:%.4f' % (test_time / (iteration + 1)))\n",
        "\n",
        "            # average\n",
        "            total_psnr =np.mean(total_psnr)\n",
        "            total_ssim =np.mean(total_ssim)\n",
        "\n",
        "            with open('valid_0006_psnr.txt', 'a') as f:\n",
        "                f.write(\"epoch: %d , PSNR: %.4f , SSIM: %.4f\\n\" % (epoch, total_psnr, total_ssim))\n",
        "                f.close()\n",
        "\n",
        "            # Print psnr, ssim\n",
        "            message = '\\t {}: {:.2f}\\t {}: {:.4f}'.format('psnr', total_psnr, 'ssim', total_ssim)\n",
        "            print(message)\n",
        "            if VISDOM == True:\n",
        "                plotter_valid.plot('loss', 'psnr', 'validation', epoch, total_psnr)\n",
        "\n",
        "\n",
        "        torch.save(netG.state_dict(), str('./checkpoints/' + METHOD + \"/netG.pkl\"))\n",
        "        torch.save(netD.state_dict(), str('./checkpoints/' + METHOD + \"/netD.pkl\"))\n",
        "        torch.save({'epoch': epoch}, str('./checkpoints/' + METHOD + \"/epoch.pkl\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}